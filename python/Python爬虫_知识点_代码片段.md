# Pythonçˆ¬è™«çŸ¥è¯†ç‚¹åŠä»£ç å®ä¾‹

## 1ã€Linuxç›¸å…³

å¿…å¤‡æŠ€èƒ½ï¼Œå‚è€ƒï¼š

[linuxåŸºç¡€çŸ¥è¯†ä¸ç³»ç»Ÿç®¡ç†](https://github.com/yaro97/note/blob/master/linux/linux%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B8%8E%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86.md)

[linuxç½‘ç»œæœåŠ¡ç®¡ç†](https://github.com/yaro97/note/blob/master/linux/linux%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86.md)

[å…¶ä»–Linuxç›¸å…³](https://github.com/yaro97/note/tree/master/linux)

## 2ã€Pythonç›¸å…³

### 2.1ã€åŸºç¡€çŸ¥è¯†

ç•¥ã€‚

### 2.2ã€é¢å‘å¯¹è±¡ï¼ˆclassï¼‰

[Pythoné¢å‘å¯¹è±¡æ€»ç»“æ¢³ç†](https://github.com/yaro97/note/blob/master/python/python_class_object-oriented_%E6%80%BB%E7%BB%93%E5%8F%8A%E6%A2%B3%E7%90%86.md)

### 2.3ã€IOç¼–ç¨‹

- æ–‡ä»¶è¯»å†™ï¼šç•¥ï¼Œå‚è€ƒï¼š[èœé¸Ÿæ•™ç¨‹](http://www.runoob.com/python/python-files-io.html)


- æ“ä½œæ–‡ä»¶å’Œç›®å½•ï¼šos/shutilæ¨¡å—


- åºåˆ—åŒ–

```python
try:
  import cPickle as pickle
except ImportError:
  import pickle
d = dict(...) 
pickle.dumps(d)  # åºåˆ—åŒ–
pickle.dump(d, f)  # åˆ°æ–‡ä»¶
# loads/loads ååºåˆ—åŒ–
# Json å¯ä»¥åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¹‹é—´ä¼ é€’æ•°æ®
```

å…¶ä»–å‚è€ƒï¼š[å»–é›ªå³°Python IOæ•™ç¨‹](https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431917590955542f9ac5f5c1479faf787ff2b028ab47000)

### 2.4ã€ç½‘ç»œç¼–ç¨‹

ç•¥ã€‚å‚è€ƒï¼š [ç½‘ç»œç¼–ç¨‹](https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014320037274136d31bd9979d648cd822375394e29a871000)

### 2.5ã€çº¿ç¨‹/è¿›ç¨‹(å¹¶å‘)

å‚è€ƒï¼š[Pythonå¹¶å‘ç›¸å…³æŠ€æœ¯](https://github.com/yaro97/note/blob/master/python/python_multiprocessing_threading_%E5%A4%9A%E7%BA%BF%E7%A8%8B_%E5%A4%9A%E8%BF%9B%E7%A8%8B.md)

### 2.6ã€ç½‘ç»œç¼–ç¨‹

ç•¥

### 2.7ã€æ•°æ®åº“ç¼–ç¨‹

ç•¥

## 3ã€Webå‰ç«¯

ç•¥ã€‚

## 4ã€å·¥å…·ç›¸å…³

[GitçŸ¥è¯†ç‚¹](https://github.com/yaro97/note/blob/master/tools/git_tutorial.md)

[Dockerç›¸å…³](https://github.com/yaro97/note/tree/master/docker)

## 5ã€çˆ¬è™«åŸºç¡€

### 5.1ã€åŸºæœ¬æ¦‚å¿µQ&A

#### çˆ¬è™«æ˜¯ä»€ä¹ˆï¼Ÿ

**è¯·æ±‚**ç½‘ç«™å¹¶**æå–**æ•°æ®çš„**è‡ªåŠ¨åŒ–**ç¨‹åº

#### çˆ¬è™«åŸºæœ¬è¿‡ç¨‹

1. å‘èµ·è¯·æ±‚ï¼šé€šè¿‡HTTPåº“å‘ç›®æ ‡ç«™ç‚¹å‘èµ·è¯·æ±‚ï¼Œå³å‘é€ä¸€ä¸ªRequestï¼Œè¯·æ±‚å¯ä»¥åŒ…å«é¢å¤–headersç­‰ä¿¡æ¯ï¼Œç­‰å¾…æœåŠ¡å™¨å“åº”ã€‚
2. è·å–ç›¸åº”å†…å®¹ï¼šå¦‚æœæœåŠ¡å™¨èƒ½æ­£å¸¸ç›¸åº”ï¼Œä¼šè¿”å›ä¸€ä¸ªResponseç»™å®¢æˆ·ç«¯ï¼ˆHTTPåº“ï¼‰ï¼ŒResponseçš„å†…å®¹ä¾¿æ˜¯æ‰€è¦è·å–çš„é¡µé¢å†…å®¹ï¼Œç±»å‹å¯èƒ½æœ‰HTMLã€Jsonå­—ç¬¦ä¸²ï¼ˆXHRï¼šXML Http Requestå¯¹è±¡ï¼Œajaxçš„æ ¸å¿ƒæŠ€æœ¯ï¼‰ã€äºŒè¿›åˆ¶æ•°æ®ï¼ˆegï¼šå›¾ç‰‡è§†é¢‘ï¼‰ç­‰ç±»å‹ã€‚
3. è§£æå†…å®¹ï¼šå¾—åˆ°çš„å†…å®¹å¯èƒ½æ˜¯HTMLï¼Œå¯ä»¥ç”¨æ­£åˆ™è¡¨è¾¾å¼ã€ç½‘é¡µè§£æåº“è¿›è¡Œè§£æï¼›å¯èƒ½æ˜¯Jsonï¼Œå¯ä»¥ç›´æ¥è½¬ä¸ºJsonå¯¹è±¡ï¼ˆdictï¼‰è§£æï¼›å¯èƒ½æ˜¯äºŒè¿›åˆ¶æ•°æ®ï¼Œå¯ä»¥åšå­˜å‚¨æˆ–è€…è¿›ä¸€æ­¥çš„å¤„ç†ã€‚
4. ä¿å­˜æ•°æ®ï¼šä¿å­˜å½¢å¼å¤šæ ·ï¼Œå¯ä»¥å­˜ä¸ºæ–‡æœ¬ï¼›ä¹Ÿå¯ä»¥ä¿å­˜è‡³æ•°æ®åº“ï¼›æˆ–è€…ä¿å­˜ç‰¹å®šæ ¼å¼çš„æ–‡ä»¶ï¼ˆå›¾ç‰‡ï¼‰ã€‚

#### Request & Response

1. HTTP Requestï¼šæµè§ˆå™¨å‘é€ä¿¡æ¯ç»™æœåŠ¡å™¨ã€‚
2. HTTP Responseï¼šæœåŠ¡å™¨æ”¶åˆ°æµè§ˆå™¨å‘é€çš„ä¿¡æ¯åï¼Œèƒ½å¤Ÿæ ¹æ®æµè§ˆå™¨å‘é€æ¶ˆæ¯çš„å†…å®¹ï¼Œåšå‡ºç›¸åº”çš„å¤„ç†ï¼Œç„¶åæŠŠæ¶ˆæ¯å›ä¼ ç»™æµè§ˆå™¨ã€‚
3. æµè§ˆå™¨æ”¶åˆ°æœåŠ¡å™¨çš„Responseä¿¡æ¯åï¼Œä¼šå¯¹ä¿¡æ¯è¿›è¡Œç›¸åº”çš„å¤„ç†ï¼Œç„¶åå±•ç¤ºã€‚

#### Requestä¸­åŒ…å«äº†ä»€ä¹ˆï¼Ÿ

1. è¯·æ±‚URLï¼šURLç»Ÿä¸€èµ„æºå®šä½ç¬¦ï¼Œå¦‚ï¼šä¸€ä¸ªç½‘é¡µç¨³å®šã€ä¸€å¼ å›¾ç‰‡ã€ä¸€ä¸ªè§†é¢‘ç­‰éƒ½å¯ä»¥ç”¨URLå”¯ä¸€æ¥ç¡®å®šã€‚
2. è¯·æ±‚æ–¹å¼ï¼šä¸»è¦æœ‰GETã€POSTä¸¤ç§ç±»å‹ï¼Œå¦å¤–è¿˜æœ‰HEADã€PUTã€DELETEã€OPTIONSç­‰ã€‚
3. è¯·æ±‚å¤´ï¼šåŒ…å«è¯·æ±‚æ—¶çš„å¤´éƒ¨ä¿¡æ¯ï¼Œå¦‚User-Agentã€Hostã€Cookiesç­‰ä¿¡æ¯ã€‚
4. è¯·æ±‚ä½“ï¼šè¯·æ±‚æ—¶é¢å¤–æºå¸¦çš„æ•°æ®ï¼ŒGETè¯·æ±‚æ—¶ä¸ä¼šæœ‰å†…å®¹ï¼Œåªæœ‰POSTè¯·æ±‚æ˜¯æ‰ä¼šæœ‰ï¼Œå¦‚è¡¨å•æäº¤æ—¶çš„è¡¨å•æ•°æ®ã€‚

#### Responseä¸­åŒ…å«äº†ä»€ä¹ˆï¼Ÿ

1. ç›¸åº”çŠ¶æ€ï¼š200=æˆåŠŸï¼Œ301è·³è½¬ï¼Œ404=æ‰¾ä¸åˆ°é¡µé¢ï¼Œ500+ï¼ˆ502ã€503...ï¼‰=æœåŠ¡å™¨é”™è¯¯ç­‰ã€‚
2. å“åº”å¤´ï¼šå¦‚å†…å®¹ç±»å‹ã€å†…å®¹é•¿åº¦ã€æœåŠ¡å™¨ä¿¡æ¯ã€è®¾ç½®Cookieç­‰ã€‚
3. å“åº”ä½“ï¼šæœ€ä¸»è¦çš„éƒ¨åˆ†ï¼ŒåŒ…å«äº†è¯·æ±‚èµ„æºçš„å†…å®¹ï¼Œå¦‚ç½‘é¡µHTMLã€å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®ç­‰ã€‚

#### çˆ¬è™«èƒ½æŠ“å–ä»€ä¹ˆæ•°æ®ï¼Ÿ

1. ç½‘é¡µæ–‡æœ¬ï¼šHTMLæ–‡æ¡£ã€Jsonæ ¼å¼æ–‡æœ¬ï¼ˆajaxï¼‰ç­‰ã€‚
2. å›¾ç‰‡ï¼šè·å–å›¾ç‰‡çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¿å­˜ä¸ºå›¾ç‰‡æ ¼å¼ã€‚
3. è§†é¢‘ï¼šåŒä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¿å­˜ä¸ºè§†é¢‘æ ¼å¼å³å¯ã€‚
4. å…¶ä»–ï¼šåªè¦æ˜¯èƒ½è¯·æ±‚åˆ°çš„ï¼Œéƒ½èƒ½è·å–ã€‚

```python
# æŠ“å–äºŒè¿›åˆ¶æ–‡ä»¶å®ä¾‹
import requests
response = requests.get('https://www.baidu.com/img/bd_logo1.png')
print(response.content)  # contentæ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œtextæ˜¯htmlæ–‡ä»¶
with open('/home/yaro/Download/1.png', 'wb') as f:
 f.write(response.content)
 f.close()  # å¯ä»¥çœç•¥
```

#### æ€æ ·è§£æå¤„ç†ï¼Ÿ

1. ç›´æ¥å¤„ç†ï¼šè¿”å›çš„æ˜¯æœ€ç®€å•çš„å­—ç¬¦ä¸²ã€‚
2. Jsonè§£æï¼ˆajaxï¼‰ï¼šè¿”å›çš„æ˜¯jsonæ ¼å¼çš„å­—ç¬¦ä¸²ã€‚
3. æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é…ã€æå–è¿”å›çš„htmlä¸­ç›¸åº”çš„æ–‡æœ¬ã€‚
4. BeautifulSoupè§£æåº“ï¼šæ›´å®¹æ˜“ã€å¥½ç”¨
5. PyQueryè§£æåº“ï¼šAPI åŒ JQuery
6. XPathè§£æåº“ï¼šBuilt-in

#### `ctrl+u`ä¸`inspect-Elementsä¸­`åŒºåˆ«ï¼Ÿ

- æ‹¿`m.weibo.com` ä¸¾ä¾‹ï¼Œåœ¨è¯·æ±‚æ—¶é¦–å…ˆè¿”å›æœ€åˆçš„docï¼ˆdomï¼‰htmlæ¡†æ¶ï¼Œç„¶åé‡Œé¢æœ‰å¾ˆå¤šJSï¼›åå°ä¼šé€šè¿‡ajaxè¯·æ±‚è¿”å›æ•°æ®ï¼Œç„¶åç”¨JSâ€œå¡«å……è‡³â€åŸhtmlä¸­ã€‚
- æˆ‘ä»¬è¯·æ±‚çš„ç»“æœä¸â€œæµè§ˆæºä»£ç ï¼ˆctrl + uï¼‰â€æ˜¯ä¸€è‡´çš„ã€‚

#### æ€ä¹ˆè§£å†³ä¸Šé¢JSæ¸²æŸ“å¯¼è‡´çš„é—®é¢˜ï¼Ÿ

ç”±äºæˆ‘ä»¬æƒ³è¦çš„ç»“æœä¸åœ¨è¯·æ±‚è¿”å›çš„ç»“æœä¸­,æˆ‘ä»¬éœ€è¦åˆ†æAjaxæˆ–è€…ä½¿ç”¨JSæ¸²æŸ“å·¥å…·;

- åˆ†æAjaxè¯·æ±‚ï¼šè¿”å›çš„æ˜¯Jsonçš„å­—ç¬¦ä¸²ã€‚
- Selenium/WebDriverï¼šæ¨¡æ‹Ÿæµè§ˆå™¨æ¸²æŸ“JSï¼Œå‘ˆç°æ¸²æŸ“åçš„ç»“æœã€‚

```python
from selenium import webdriver
driver = webdriver.Chrome()
driver.get('http://www.taobao.com')
print(driver.page_source)  # è¿™ä¸ªæ•°æ®æ˜¯ä¸å¯èƒ½ç”¨è¯·æ±‚åº“æ‹¿åˆ°çš„æ•°æ®ï¼Œè¿™é‡Œé¢çš„æ•°æ®éƒ½æ˜¯JSæ¸²æŸ“åçš„æ•°æ®ï¼Œå¯ä»¥ç›´æ¥è§£ææå–ã€‚
```

- Splashåº“ï¼šä¹Ÿæ˜¯æ¨¡æ‹ŸJSæ¸²æŸ“çš„ã€‚
- PyV8ã€Ghost.pyç­‰å…¶ä»–åº“æ¥æ¨¡æ‹ŸåŠ è½½

#### æ€ä¹ˆæ•°æ®æŒä¹…åŒ–ï¼ˆå­˜å‚¨ï¼‰ï¼Ÿ

1. æ–‡æœ¬ï¼šçº¯æ–‡æœ¬ã€Jsonã€Xmlç­‰ã€‚
2. å…³ç³»å‹æ•°æ®åº“ï¼šå¦‚MySQLã€Oracleã€SQL Serverç­‰å…·æœ‰ç»“æ„åŒ–è¡¨ç»“æ„å½¢å¼å­˜å‚¨ã€‚
3. å¦‚MongoDBã€Redisç­‰Key-Valueå½¢å¼å­˜å‚¨ã€‚
4. äºŒè¿›åˆ¶æ–‡ä»¶ï¼šå›¾ç‰‡ã€è§†é¢‘ã€éŸ³é¢‘ç­‰ç›´æ¥ä¿å­˜æˆç‰¹å®šçš„æ ¼å¼å³å¯ã€‚

### 5.2ã€çˆ¬è™«å¸¸ç”¨åº“/å·¥å…·

#### å·¥å…·æ€»è§ˆ

çˆ¬è™«ä½¿ç”¨åˆ°çš„pythonåº“çš„æœ‰ï¼šè¯·æ±‚åº“ã€è§£æåº“ã€å­˜å‚¨åº“ã€å·¥å…·åº“ï¼›

è¯·æ±‚åº“ï¼šurllib, requests, selenium

è§£æåº“ï¼š lxml, beautifulsoup, pyquery

å­˜å‚¨åº“ï¼špymysql, pymongo, redis

å¸¸ç”¨å·¥å…·ï¼šipython, jupyter, RE, Json, Chrome, Phantomjs, Mysql, MongoDB, Redis, Sublime_text, Pycharmç­‰

#### Urllibåº“åŸºæœ¬ä½¿ç”¨

pythonè‡ªå¸¦çš„è¯·æ±‚åº“,ä¸‹é¢çš„Requestsæ˜¯æ­¤åº“çš„åŒ…è£…;

#### Requestsåº“åŸºæœ¬ä½¿ç”¨

è§ã€Šrequests.mdã€‹

#### Re æ­£åˆ™è¡¨è¾¾å¼

é¡¹ç›®å‚è€ƒï¼š[MaoYan100_re](https://github.com/yaro97/spider_projects/blob/master/MaoYan100_re/main.py)

æ–¹ä¾¿åœ°æå–htmlä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚

**å½“æˆ‘ä»¬éœ€è¦æå–çš„ä¿¡æ¯åœ¨htmlæºç ä¸­ï¼Œä½†æ˜¯ä¸åœ¨æ ‡ç­¾ä¸­ï¼Œåªå­˜åœ¨ä¸JSçš„ç›¸å…³å˜é‡ä¸­æ—¶**ï¼ˆä»Šæ—¥å¤´æ¡-å›¾ç‰‡ï¼‰ï¼Œå°±ä¸èƒ½ä½¿ç”¨Beautifulsoupï¼ŒPyqueryç­‰è§£æåº“æå–äº†ã€‚æ­£åˆ™è¡¨è¾¾å¼å°±å¾ˆæ–¹ä¾¿äº†ã€‚

**re æ¨¡å—çš„å¸¸ç”¨å‡½æ•°**

| ç”¨é€” | å‡½æ•°å£°æ˜                                          | è¿”å›å€¼           |
| ---- | ------------------------------------------------- | ---------------- |
| ç¼–è¯‘ | `re.compile(pattern, flags=0)`                    | `re.RegexObject` |
| æŸ¥æ‰¾ | `re.search(pattern, string, flags=0)`             | `re.MatchObject` |
| æŸ¥æ‰¾ | `re.findall(pattern, string, flags=0)`            | `list`           |
| æŸ¥æ‰¾ | `re.finditer(pattern, string, flags=0)`           | `iterator`       |
| æ›¿æ¢ | `re.sub(pattern, repl, string, count=0, flags=0)` | `string`         |
| åˆ†å‰² | `re.split(pattern, string, maxsplit=0, flags=0)`  | `list`           |

å…¶ä¸­ï¼Œæ¯”è¾ƒå¸¸ç”¨çš„æ˜¯ `re.sub`ã€`re.split` å’Œ `re.findall`ã€‚

`re.compile` è¿”å›çš„å¯¹è±¡ `re.RegexObject` ä¹Ÿæ”¯æŒ re æ¨¡å—ä¸‹çš„å¤§éƒ¨åˆ†å‡½æ•°ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œä½ å¯ä»¥è¿™æ ·è°ƒç”¨

```python
import re
content = 'price is $5.20, hahaha'  # è®°å¾—è½¬ä¹‰ $ .
result = re.match('pattern', content, re.S) #pattern content flag
print(result)
print(result.group(1)) # 0 æˆ–è€… ç©º ï¼Œéƒ½ä»£è¡¨æ‰€æœ‰å­—ç¬¦
# re.matchæ˜¯ä»ç¬¬ä¸€ä¸ªå­—ç¬¦åŒ¹é…çš„ï¼Œç¬¬ä¸€ä¸ªå­—ç¬¦ä¸ç¬¦åˆå°±åŒ¹é…ä¸æˆåŠŸï¼Œä½¿ç”¨èµ·æ¥ä¸æ–¹ä¾¿ã€‚
# re.serachå°±æ–¹ä¾¿çš„å¤š-æœç´¢æ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå¤šä¸ªæˆåŠŸåŒ¹é…,åªè¿”å›ç¬¬ä¸€ä¸ªï¼›
pattern = re.compile('xxx.*xx', re.S)
result = pattern.search(content)  # å¯ä»¥é‡å¤ä½¿ç”¨,èŠ‚çº¦compileæ—¶é—´;
# result = re.search(pattern, conntent)
```

ä½¿ç”¨ `re.compile` ä¼šæå‰ç¼–è¯‘ç»™å®šçš„ REï¼Œé¿å…æ¯æ¬¡éƒ½é‡æ–°ç¼–è¯‘ï¼›ä¸è¿‡ Python ä¹Ÿæ²¡æœ‰é‚£ä¹ˆè ¢ï¼Œå®ƒä¼šç¼“å­˜ç¼–è¯‘è¿‡çš„ REã€‚

> `re.match()`çš„ç”¨é€”æ¯”è¾ƒçª„ï¼Œä¸€èˆ¬åªç”¨æ¥åš**å®Œå…¨åŒ¹é…**ï¼Œå…³äºå®ƒä¸ `re.search`ï¼Œå‚è€ƒ [search() vs. match()](https://docs.python.org/3/library/re.html#search-vs-match)
>
> å› ä¸ºå‡ ä¹ç”¨ä¸åˆ°å®ƒï¼Œæ‰€ä»¥æœ€å¥½åˆ«è®°ä½å®ƒï¼Œä¸ç„¶ä½ æ¯æ¬¡éƒ½ä¼šçº ç»“å®ƒè·Ÿ `re.search` æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚

**ä½¿ç”¨ç¤ºä¾‹**

ä¸­æ–‡è¯­æ–™æ¸…æ´—

æ•°æ®æ ¼å¼ï¼š

```python
["text1", "text2", "text3", ...]
```

ä»£ç 

```python
data = ["è¿‘æ—¥å—äº¬å¸‚ä¸‹å‘ã€Šå…³äºåŠ å¼ºå‡ºç§Ÿæ±½è½¦å¸‚åœºè§„èŒƒç®¡ç†çš„æ„è§ã€‹ï¼ˆä»£æ‹Ÿç¨¿ï¼‰ï¼ŒæŒ‰ç…§ã€Šæ„è§ã€‹è§„å®š\n",
        "Outlook 2010ä¸­æ–°å¢äº†ä¸€ä¸ªâ€œäººç‰©çª—æ ¼â€ï¼ˆPeople Paneï¼‰åŠŸèƒ½ï¼Œ\n",
        "Steamä¸Šå·²ç»å‘å¸ƒäº†è¶…è¿‡6800æ¬¾æ¸¸æˆï¼Œè€Œ2016å¹´è¿™ä¸ªæ•°å­—ä¸º5028ï¼Œ2015å¹´ä¸º2991æ¬¾ã€‚\n"]

RE_zh_en = re.compile(r"[^\u4e00-\u9fa5a-zA-Z]")
"""åŒ¹é…æ‰€æœ‰éä¸­è‹±æ–‡å­—ç¬¦"""
RE_white_space = re.compile(r"(\s)+")
"""åŒ¹é…è¿ç»­çš„ç©ºç™½ç¬¦"""

for line in data:
    # æŠŠé™¤ä¸­æ–‡ã€è‹±æ–‡å¤–çš„å­—ç¬¦è½¬ä¸ºç©ºæ ¼
    line = RE_zh_en.sub(' ', line)
    # å»æ‰é¦–å°¾çš„ç©ºæ ¼
    line = line.strip()
    # æŠŠå¤šä¸ªç©ºç™½ç¬¦è½¬ç©ºæ ¼
    line = RE_white_space.sub(' ', line)
    print(line)
```

å…·ä½“çš„è¦åšå“ªäº›å¤„ç†è§†æ•°æ®è€Œå®šï¼Œ
ä¸€ä¸ªå¤æ‚çš„ä¾‹å­ï¼šgensim å¤„ç† wiki è¯­æ–™

å¤„ç†ç»“æœ

```sh
è¿‘æ—¥å—äº¬å¸‚ä¸‹å‘ å…³äºåŠ å¼ºå‡ºç§Ÿæ±½è½¦å¸‚åœºè§„èŒƒç®¡ç†çš„æ„è§ ä»£æ‹Ÿç¨¿ æŒ‰ç…§ æ„è§ è§„å®š
Outlook ä¸­æ–°å¢äº†ä¸€ä¸ª äººç‰©çª—æ ¼ People Pane åŠŸèƒ½
Steamä¸Šå·²ç»å‘å¸ƒäº†è¶…è¿‡ æ¬¾æ¸¸æˆ è€Œ å¹´è¿™ä¸ªæ•°å­—ä¸º å¹´ä¸º æ¬¾
```

æ•°æ®æ¸…ç†åï¼Œä¸‹ä¸€æ­¥å°±æ˜¯**åˆ†è¯**äº†ï¼Œæœªå®Œå¾…ç»­ã€‚

**æ€»ç»“ï¼š**

- èƒ½ç”¨`å­—ç¬¦ä¸².Method`,å°½é‡ä¸ç”¨RE,æ¯”å¦‚å­—ç¬¦ä¸²çš„`split()`æ–¹æ³•,æ— æ³•å®ç°ç”¨ä¸åŒçš„`åˆ†å‰²ç¬¦`åˆ†å‰²,REå¯ä»¥; 
- å°½é‡ä½¿ç”¨æ³›åŒ¹é…ï¼ˆ `.*` ä»£æ›¿æ¯”è¾ƒé•¿çš„å­—ç¬¦ï¼‰ï¼›
- patternä¸­ä½¿ç”¨æ‹¬å·è·å–åŒ¹é…ç›®æ ‡(ç»„),0æˆ–è€…ç©ºä»£è¡¨æ‰€æœ‰ç»„ï¼›
- å°½é‡ä½¿ç”¨éè´ªå©ªæ¨¡å¼(.*?)ï¼›
- æœ‰æ¢è¡Œç¬¦å°±æ˜¯ç”¨ `re.S` Flag(è®© . å¯ä»¥åŒ¹é…æ¢è¡Œç¬¦)ï¼›
- å°½é‡ä½¿ç”¨searchä»£æ›¿match, re.serachæœç´¢æ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå¤šä¸ªæˆåŠŸåŒ¹é…,åª**è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…**ï¼›
- re.findallæŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆçš„å­—ç¬¦ä¸²ï¼Œè¿”å›`ä¸€ä¸ªåˆ—è¡¨`ï¼Œé‡Œé¢æ˜¯`å„ä¸ªåŒ¹é…ç»“æœç»„æˆçš„å…ƒç»„`ï¼›
- re.sub('\d+', 'Replacement', content) å°†contentä¸­åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²ç”¨Replacementæ›¿æ¢,è¿”å›æ›¿æ¢åçš„ç»“æœï¼›
- re.sub('(\d+)', r'\1 Replacement', content) å¯ä»¥å®ç°åœ¨æºå­—ç¬¦ä¸²åé¢è¿½åŠ , `\1` åˆ«æ˜¯å‰é¢çš„åˆ†ç»„`(\d+)`;
- ä¹ æƒ¯ä½¿ç”¨`res = re.compile(pattern, re.S), res.search('content')`,åœ¨é‡å¤compileæ—¶,æ¯”`re.search(pattern, content)`æ›´é«˜æ•ˆ;

#### Lxml - xpathè§£æåº“

è¯¦ç»†è§ã€Šxml.mdã€‹

æä¾›äº†xpathè§£ææ–¹å¼ï¼ˆæ–¹ä¾¿ï¼Œé«˜æ•ˆï¼‰

```python
pip install lxml
from lxml import etree
import requests
url = 'http://www.baidu.com'
response = requests.get(url)  # requests.models.Responseå¯¹è±¡, Response.textå±æ€§æ˜¯htmlçš„å­—ç¬¦ä¸²ç±»å‹
html = etree.HTML(response.text)  #å°†å­—ç¬¦ä¸²ç±»å‹è½¬æ¢ä¸ºlxml.etree._Elementç±»å‹
result = html.xpath('//title/text()')[0] #å¯ä»¥å¯¹htmlä½¿ç”¨xpathæ–¹æ³•è¿›è¡Œè§£æ
```

> æœ‰æ—¶æˆ‘ä»¬ç”±äºç½‘ç»œé—®é¢˜é€ æˆå®‰è£…åº“å›°éš¾çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹æ³•è§£å†³ï¼š1ã€googleç›¸åº”çš„åº“åï¼Œè¿›å…¥pypi.python.orgç›¸åº”çš„åº“é¡µé¢ï¼Œä¸‹è½½.whlæ–‡ä»¶ï¼ˆå‰æéœ€è¦å®‰è£…wheelåº“ï¼‰ï¼Œç„¶åç”¨ `pip install whlæ–‡ä»¶è·¯å¾„` å®‰è£…å³å¯ ï¼›2ã€ä½¿ç”¨å›½å†…çš„ç›¸å…³é•œåƒ

#### PyQueryè¯¦è§£

ç•¥ï¼Œè¯¦ç»†è§ã€Špyquery.mdã€‹

#### Beautifulsoup4

ç•¥ï¼Œè¯¦ç»†è§ã€Šbeautifulsoup4.mdã€‹

#### Seleniumè¯¦è§£

ç•¥ï¼Œè¯¦ç»†è§ã€Šselenium.mdã€‹

#### Pymysql - è¿æ¥Mysql

ç”¨äºæ“ä½œmysqlæ•°æ®åº“; python2ä½¿ç”¨`mysql-python`åº“ï¼›åœ¨python3ä¸­ä½¿ç”¨`pymysql`ä»£æ›¿ã€‚

```python
pip install pymysql
import pymysql
conn = pymysql.connect(host='localhost', user='root', password='123456',port='3306', db='mysql')  # å£°æ˜ä¸€ä¸ªè¿æ¥å¯¹è±¡
cursor = conn.cursor()  # è°ƒç”¨connçš„cursoræ–¹æ³•ï¼ŒæŒ‡å®šæ•°æ®åº“çš„æ“ä½œå¯¹è±¡
cursor.execute('select * from db')  # è°ƒç”¨ä¸€äº›å‡½æ•°æ‰§è¡Œsqlè¯­å¥
cursor.fetchone()  # ä½¿ç”¨fetchoneæ–¹æ³•æ‹¿åˆ°æ•°æ®åº“çš„å†…å®¹
```

#### Pymongo - è¿æ¥MongoDB

é¡¹ç›®å‚è€ƒï¼š[TaoBaoGoods_selenium](https://github.com/yaro97/spider_projects/blob/master/TaoBaoGoods_selenium/main.py)

ç”¨äºæ“ä½œmongodbæ•°æ®åº“ï¼Œæ˜¯key:valueå½¢å¼ï¼ˆjsonï¼Œç±»ä¼¼ä¸pythonä¸­çš„å­—å…¸ï¼‰çš„éå…³ç³»å‹æ•°æ®åº“ï¼Œåœ¨çˆ¬è™«çš„æ•°æ®å­˜å‚¨æ—¶ï¼Œä½¿ç”¨mongodbæ¥å­˜å‚¨æ˜¯éå¸¸æ–¹ä¾¿çš„ï¼›ä¸éœ€è¦å»ºè¡¨ï¼Œä¸éœ€è¦å…³ç³»è¡¨çš„ç»“æ„ï¼Œè€Œä¸”å¯ä»¥åŠ¨æ€çš„å¢åŠ é”®åã€‚

```python
pip install pymongo
# å¯åŠ¨mongodbæ•°æ®åº“
import pymongo
client = pymongo.MongoClient('localhost')  # å£°æ˜ä¸€ä¸ªMongoDBçš„è¿æ¥å¯¹è±¡
db = client['testdb']  # è°ƒç”¨clientå£°æ˜ä¸€ä¸ªæ•°æ®åº“
db['table'].insert({'name':'Bob'})  # è°ƒç”¨dbå£°æ˜ä¸€ä¸ªè¡¨å,å¹¶æ’å…¥ä¸€æ¡æ•°æ®
db['table'].find_one({'name':'Bob'})  # æ•°æ®çš„æŸ¥è¯¢
```

#### Redis - è¿æ¥Redis

ä¹Ÿæ˜¯éå…³ç³»å‹æ•°æ®åº“ï¼Œä¹Ÿæ˜¯key:valueå½¢å¼å­˜å‚¨çš„ã€‚ä¸»è¦åº”ç”¨äºåé¢çš„åˆ†å¸ƒå¼çˆ¬è™«ã€ç»´æŠ¤ä¸€ä¸ªçˆ¬å–é˜Ÿåˆ—ï¼Œè¿è¡Œæ•ˆç‡æ¯”è¾ƒé«˜ï¼Œç”¨å®ƒç»´æŠ¤ä¸€ä¸ªå…¬å…±çˆ¬å–é˜Ÿåˆ—ï¼Œæ•ˆæœæ¯”è¾ƒå¥½ã€‚

```python
pip install redis
import redis
r = redis.Redis('localhost', 6379)  #  å£°æ˜ä¸€ä¸ªrediså¯¹è±¡
r.set('name', 'Bob')  # æ’å…¥æ•°æ®
r.get('name')  # è·å–æ•°æ®
```

#### Phantomjs - æ— ç•Œé¢æµè§ˆå™¨

é¡¹ç›®å‚è€ƒï¼š[TaoBaoGoods_selenium](https://github.com/yaro97/spider_projects/blob/master/TaoBaoGoods_selenium/main.py)

â€‹ åœ¨åšçˆ¬è™«æ—¶ï¼Œä¸€ç›´å‡ºç°ä¸€ä¸ªæµè§ˆå™¨æ˜¯å¾ˆä¸æ–¹ä¾¿çš„ï¼›è¿™æ˜¯æˆ‘ä»¬å°±éœ€è¦ä¸€ä¸ªæ²¡æœ‰ç•Œé¢çš„æµè§ˆå™¨â€”â€”phantomjsã€‚

[å®˜ç½‘](http://phantomjs.org/), ä¸‹è½½è§£å‹åï¼Œå°†/binå­ç›®å½•é…ç½®åˆ°ç¯å¢ƒå˜é‡å³å¯ã€‚

```python
# ç›´æ¥åœ¨cmdä¸‹è¿è¡Œ
phantomjs
console.log('Hello World')  # ç›¸å½“äºå¯åŠ¨ä¸€ä¸ªæ§åˆ¶å°
# åœ¨pythonä¸‹è¿è¡Œ
from selenium import webdriver
driver.get('http://www.baidu.com')
driver = webdriver.PhantomJS()
driver.page_source  # åŒChrome
```

#### Flask - ç®€å•webæ¡†æ¶

æ˜¯ä¸€ä¸ªwebåº“ï¼Œåé¢çš„ä»£ç†è®¾ç½®ï¼ˆä»£ç†çš„è·å–ã€å­˜å‚¨ï¼‰ä¼šç”¨åˆ°ã€‚

```python
pip install flask  # ä¾èµ–çš„åº“æ¯”è¾ƒå¤š
```

#### Django - å¼ºå¤§webæ¡†æ¶

æ˜¯ä¸€ä¸ªwebæœåŠ¡å™¨æ¡†æ¶ï¼Œæä¾›äº†å®Œæ•´çš„åå°ç®¡ç†ï¼Œæä¾›äº†æ¨¡æ¿å¼•æ“ã€æ¥å£ã€è·¯ç”±ç­‰ã€‚å¯ä»¥ä½¿ç”¨djangoåšä¸€ä¸ªå®Œæ•´çš„ç½‘ç«™ã€‚åé¢çš„åˆ†å¸ƒå¼çˆ¬è™«ç»´æŠ¤æ—¶ä¼šç”¨åˆ°ã€‚

```python
pip install django  # ä¾èµ–çš„åº“æ¯”è¾ƒå¤š
```

#### Jupyter - pythonè®°äº‹æœ¬

è¿è¡Œåœ¨ç½‘é¡µç«¯çš„è®°äº‹æœ¬ï¼Œå¯ä»¥è°ƒè¯•å‘½ä»¤ï¼Œæ”¯æŒmkè¯­æ³•ã€‚

```python
pip install jupyter  # ä¾èµ–çš„åº“æ¯”è¾ƒå¤š
jupyter notebook  
```

#### MongoDBç¯å¢ƒé…ç½®

windowså»ºè®®ä½¿ç”¨--installæ·»åŠ æœåŠ¡ï¼ŒGUIæ•°æ®æŸ¥çœ‹å»ºè®®ä½¿ç”¨robo3t;

```mysql
mongod --bind_ip 0.0.0.0 --logpath D:\Dev\MongoDB\Server\3.6\data\logs\mongo.log --logappend --dbpath D:\Dev\MongoDB\Server\3.6\data\db --port 27017 --serviceName "MongoDB" --serviceDisplayName "MongoDB" --install
# éœ€è¦æå‰åˆ›å»ºæ•°æ®, logæ–‡ä»¶å¤¹;
db;
show dbs;
db.test.insert({'a':'b'})
```

#### Redisç¯å¢ƒé…ç½®

å†…å­˜ä¸­çš„NoSQLæ•°æ®åº“,é«˜æ•ˆ,åé¢åˆ†å¸ƒå¼çˆ¬è™«ä¼šç”¨åˆ°;

å®‰è£…: linuxç›´æ¥REPOä¸‹è½½å³å¯; Winç‰ˆæœ¬å¤ªå¤š,å»ºè®®ä½¿ç”¨[å¾®è½¯çš„Reidsç‰ˆæœ¬](https://github.com/MicrosoftArchive/redis/releases)

GUIå¯è§†åŒ–ç®¡ç†è½¯ä»¶æ¨è:[RedisDesktopManager](https://github.com/uglide/RedisDesktopManager)

linuxä¸‹ä½¿ç”¨`redis-cli`å‘½ä»¤å¯åŠ¨;

è®°å¾—é…ç½®redisé…ç½®æ–‡ä»¶`vim /etc/redis/redis.conf`

```sh
bind 127.0.0.1ï¼ˆåªå…è®¸æœ¬åœ°ï¼‰
requirepass foobar å¯ç”¨å¯†ç 
redis-cli -a foobar # ä½¿ç”¨å¯†ç ç™»é™†
....
```

ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼šå–æ¶ˆé™åˆ¶åªæœ‰æœ¬åœ°è¿æ¥ï¼Œæ·»åŠ å¯†ç 

#### MySQLç¯å¢ƒé…ç½®

å¯ä»¥ä½¿ç”¨MariaDBä»£æ›¿;

```mysql
mysql -uroot -p 
# å¯ä»¥ä½¿ç”¨-hå‚æ•°ï¼Œå‚æ•°åé¢å¯ä»¥æœ‰ç©ºæ ¼ï¼Œå¯ä»¥æ²¡æœ‰ç©ºæ ¼ï¼›
# ä½†æ˜¯-påé¢ä¸èƒ½æœ‰ç©ºæ ¼ï¼Œå¦åˆ™éœ€è¦é‡æ–°è¾“å…¥å¯†ç ã€‚
```

winä¸‹GUIç®¡ç†å·¥å…·ï¼šNavicat/MySQL Frontç­‰ç­‰ã€‚

#### å„å·¥å…·çš„æ•ˆç‡/é€‰æ‹©çš„æ€è€ƒ

è§£æåº“ä¸­ï¼ŒPyqueryå’ŒBeautifulsoup4éƒ½æ˜¯åŸºäºLxmlåº“çš„ï¼Œç½‘ä¸Šå¾ˆå¤šäººè¯æ˜Lxmlåº“çš„æ•ˆç‡æ›´é«˜!

å½“ç„¶ï¼ŒPyqueryå’ŒBeautifulsoup4æ—¢ç„¶åŒ…è£…äº†Lxml,æ˜“ç”¨æ€§è‚¯å®šæ›´é«˜ï¼

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•é€‰æ‹©å‘¢,é€‰æ‹©æ˜“ç”¨æ€§ or æ•ˆç‡ ï¼Ÿ

é¦–å…ˆï¼Œå¼•å…¥pythonå®ç°å¹¶è¡Œä»»åŠ¡çš„ç›¸å…³æ¦‚å¿µï¼š

ç”±äºCPythonçš„GILçš„åŸå› ï¼Œpythonæ— æ³•å®ç°å¤šçº¿ç¨‹,å¯¼è‡´CPUå¯†é›†å‹ä»»åŠ¡ä½¿ç”¨å¤šçº¿ç¨‹åè€Œé™ä½æ•ˆç‡,ä½†æ˜¯åœ¨I/Oå¯†é›†å‹çš„ä»»åŠ¡ä¸Šï¼Œå¤šçº¿ç¨‹(Threading)ä¾ç„¶å¯ä»¥æœ‰æ•ˆçš„æå‡æ•ˆç‡ï¼

ä¸ºä»€ä¹ˆè¯´è¿™ä¸ªå‘¢ï¼Ÿå¯¹äºä»»ä½•è®¡ç®—æœºå®ç°çš„ä»»åŠ¡,æˆ‘ä»¬éƒ½å¯ä»¥è¿™ä¹ˆç†è§£ï¼š

ä¸ºäº†å®ç°ä¸€ä¸ªç½‘ç»œä»»åŠ¡ï¼Œè®¡ç®—æœºéœ€è¦ **è”ç½‘** - **è®¿é—®æ•°æ®** - **ä¸‹è½½æ•°æ®** - **è§£ææ•°æ®** - **ä¿å­˜æ•°æ®(I/O)** ï¼Œ è€Œåœ¨æ‰€æœ‰çš„è¿‡ç¨‹ä¸­ï¼Œéƒ½éœ€è¦**CPUè®¡ç®—**, æ•´ä¸ªç½‘ç»œä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´(æ•ˆç‡)å–å†³äºæ•ˆç‡æœ€ä½(æ—¶é—´æœ€å¤š)çš„é‚£ä¸ªç¯èŠ‚ï¼

Soï¼Œè¯·æ±‚åº“/è§£æåº“çš„æ•ˆç‡çœŸçš„æ˜¯ä½ ç‰¹åˆ«éœ€è¦è§£å†³çš„ä¹ˆï¼Ÿ ç›¸ä¿¡ä½ å·²ç»æœ‰äº†ç­”æ¡ˆ!ğŸ˜„

## 5ã€Scrapyæ¡†æ¶

å‚è€ƒï¼š[å®˜ç½‘](https://docs.scrapy.org/en/latest/)  [Scrapyæ•™ç¨‹](http://blog.csdn.net/Inke88/article/details/60573507) [Learn Scrapy](https://learn.scrapinghub.com/scrapy/)

Scrapyï¼ŒPythonå¼€å‘çš„ä¸€ä¸ªå¿«é€Ÿ,é«˜å±‚æ¬¡çš„webæŠ“å–æ¡†æ¶ï¼›

- Scrapyæ˜¯ä¸€ä¸ªä¸ºäº†çˆ¬å–ç½‘ç«™æ•°æ®ï¼Œæå–ç»“æ„æ€§æ•°æ®è€Œç¼–å†™çš„åº”ç”¨æ¡†æ¶ã€‚
- å…¶æœ€åˆæ˜¯ä¸ºäº†é¡µé¢æŠ“å– (æ›´ç¡®åˆ‡æ¥è¯´, ç½‘ç»œæŠ“å– )æ‰€è®¾è®¡çš„ï¼Œä¹Ÿå¯ä»¥åº”ç”¨åœ¨è·å–APIæ‰€è¿”å›çš„æ•°æ®(ä¾‹å¦‚ Amazon Associates Web Services ) æˆ–è€…é€šç”¨çš„ç½‘ç»œçˆ¬è™«ã€‚
- Scrapyç”¨é€”å¹¿æ³›,å¯ä»¥ç”¨äºæ•°æ®æŒ–æ˜ã€ç›‘æµ‹å’Œè‡ªåŠ¨åŒ–æµ‹è¯•
- Scrapyä½¿ç”¨äº†Twisted å¼‚æ­¥ç½‘ç»œåº“æ¥å¤„ç†ç½‘ç»œé€šè®¯ã€‚

### 5.1ã€æ¶æ„å’Œæ•°æ®æµ

![](https://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png)

1. The [Engine](https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine) gets the initial Requests to crawl from the [Spider](https://doc.scrapy.org/en/latest/topics/architecture.html#component-spiders).
2. The [Engine](https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine) schedules the Requests in the [Scheduler](https://doc.scrapy.org/en/latest/topics/architecture.html#component-scheduler) and asks for the next Requests to crawl.
3. The [Scheduler](https://doc.scrapy.org/en/latest/topics/architecture.html#component-scheduler) returns the next Requests to the [Engine](https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine).
4. The [Engine](https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine) sends the Requests to the [Downloader](https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader), passing through the [Downloader Middlewares](https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader-middleware) (see [`process_request()`](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request)).
5. Once the page finishes downloading the [Downloader](https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader) generates a Response (with that page) and sends it to the Engine, passing through the [Downloader Middlewares](https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader-middleware) (see [`process_response()`](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response)).
6. The [Engine](https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine) receives the Response from the [Downloader](https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader) and sends it to the [Spider](https://doc.scrapy.org/en/latest/topics/architecture.html#component-spiders) for processing, passing through the [Spider Middleware](https://doc.scrapy.org/en/latest/topics/architecture.html#component-spider-middleware) (see [`process_spider_input()`](https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input)).
7. The [Spider](https://doc.scrapy.org/en/latest/topics/architecture.html#component-spiders) processes the Response and returns scraped items and new Requests (to follow) to the [Engine](https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine), passing through the [Spider Middleware](https://doc.scrapy.org/en/latest/topics/architecture.html#component-spider-middleware) (see [`process_spider_output()`](https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output)).
8. The [Engine](https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine) sends processed items to [Item Pipelines](https://doc.scrapy.org/en/latest/topics/architecture.html#component-pipelines), then send processed Requests to the [Scheduler](https://doc.scrapy.org/en/latest/topics/architecture.html#component-scheduler) and asks for possible next Requests to crawl.
9. The process repeats (from step 1) until there are no more requests from the [Scheduler](https://doc.scrapy.org/en/latest/topics/architecture.html#component-scheduler).

å…¶ä»–å‚è€ƒï¼š[Architecture Overview](https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine)

### 5.2ã€åŸºæœ¬ä½¿ç”¨è¿‡ç¨‹

å‚è€ƒé¡¹ç›®ï¼š[è…¾è®¯hræ‹›è˜ä¿¡æ¯](https://github.com/yaro97/spider_projects/tree/master/TencentHR_scrapy)

#### aã€åˆ›å»ºé¡¹ç›®ï¼š

`scrapy startproject turoial`

ç”Ÿæˆé¡¹ç›®ç›®å½•å¦‚ä¸‹ï¼š

```
scrapy.cfg:         é¡¹ç›®çš„é…ç½®æ–‡ä»¶
tutorial/:          è¯¥é¡¹ç›®çš„pythonæ¨¡å—, åœ¨è¿™é‡Œæ·»åŠ ä»£ç 
    items.py:       é¡¹ç›®ä¸­çš„itemæ–‡ä»¶
    pipelines.py:   é¡¹ç›®ä¸­çš„pipelinesæ–‡ä»¶.
    settings.py:    é¡¹ç›®å…¨å±€è®¾ç½®æ–‡ä»¶.
    spiders/        çˆ¬è™«æ¨¡å—ç›®å½•
```

#### bã€åˆ›å»ºçˆ¬è™«ï¼š

`scrapy genspider xxx www.xxx.com`

#### cã€å®šä¹‰itemï¼š

å³ï¼Œä½ æƒ³è·å¾—çš„æ•°æ®ã€‚Item å®šä¹‰ç»“æ„åŒ–æ•°æ®å­—æ®µï¼Œç”¨æ¥ä¿å­˜çˆ¬å–åˆ°çš„æ•°æ®ï¼›å…¶ä½¿ç”¨æ–¹æ³•å’Œpythonå­—å…¸ç±»ä¼¼

å¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ª `scrapy.Item` ç±»ï¼Œ å¹¶ä¸”å®šä¹‰ç±»å‹ä¸º `scrapy.Field`çš„ç±»å±æ€§æ¥å®šä¹‰ä¸€ä¸ªItemã€‚

```python
import scrapy
class QuotesbotItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # èŒä½å
    positionName = scrapy.Field()
    # èŒä½è¯¦æƒ…é“¾æ¥
    positionLink = scrapy.Field()
    # èŒä½ç±»åˆ«
    positionType = scrapy.Field()
	...
```

#### dã€ç¼–å†™çˆ¬è™«æ–‡ä»¶ï¼š

parseæ–¹æ³•`yield item` å’Œ `yield scrapy.Request`

```python
import scrapy
from TencentHR_scrapy.items import TencentHrScrapyItem
class TecentHrSpider(scrapy.Spider):
    name = "tencent_hr"
    allowed_domains = ["tencent.com"]
    base_url = 'http://hr.tencent.com/position.php?&start='
    offset = 0
    start_urls = [base_url + str(offset)]  # æ„å»ºç¬¬ä¸€é¡µ

    def parse(self, response):
        positions = response.css('#position .even,.odd')  # æå–å•é¡µæ‰€æœ‰èŒä½ä¿¡æ¯
        for position in positions:
            item = TencentHrScrapyItem()  # æ„å»ºitemå¯¹è±¡ï¼Œç”¨æ¥ä¿å­˜æ•°æ®
            # æå–æ¯ä¸ªèŒä½çš„å…·ä½“ä¿¡æ¯
            item['positionName'] = position.css('td:nth-child(1) a::text').extract()[0]
            item['peopleNumber'] = position.css('td:nth-child(3)::text').extract()[0]
            ...

        if not response.css('.tablelist .pagenav .noactive#next'):  # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µ
            self.offset += 10
            url = self.base_url + str(self.offset)
            yield scrapy.Request(url, callback=self.parse)  # è°ƒç”¨å›è°ƒå‡½æ•°parseè§£æä¸‹ä¸€é¡µ
```

> çˆ¬è™«æ–‡ä»¶ä½¿ç”¨å¯ä»¥ä½¿ç”¨Scrapyè‡ªå¸¦çš„Selectorsè§£ææ•°æ®(xpath/css/re/extract)ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ç¬¬ä¸‰æ–¹åº“è§£æï¼ˆlxml/pyquery/beautifulsoup4ï¼‰ï¼›è¿‡ç¨‹ä¸­å¯ä»¥ä½¿ç”¨`scrapy shell http://xxx`æ¥è°ƒè¯•ã€‚

#### eã€å¤„ç†/å­˜å‚¨æ•°æ®ï¼š

1ã€å¯ä»¥ä½¿ç”¨è‡ªå¸¦çš„`Feed exports`åˆ°å¤„æ•°æ®æ–‡ä»¶ï¼›

2ã€ä¹Ÿå¯ä»¥ç¼–å†™Pipelineæ–‡ä»¶ï¼šå¤„ç†itemï¼Œ å†™å…¥/æ–‡ä»¶æ•°æ®åº“ç­‰...

#### fã€æ‰§è¡Œçˆ¬è™«ï¼š

`scrapy crawl xxx`ï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥é€šè¿‡ä»£ç æ–‡ä»¶`run.py`è¿è¡Œçˆ¬è™«ï¼Œå‚è€ƒ[å®˜ç½‘](https://doc.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script)

### 5.3ã€æ·±å…¥ä»‹ç»

#### aã€å…³äºçˆ¬è™«æ–‡ä»¶

1ã€scrapy.Spiderç±»

å¦‚ä¸Šé¢çš„`TecentHrSpider`ç±»ï¼Œç»§æ‰¿äº`scrapy.Spider`ç±»ï¼Œç”¨äºæ„é€ Requestå¯¹è±¡ç»™Schedulerã€‚

å¸¸ç”¨å±æ€§å’Œæ–¹æ³•ï¼š

- `name`ï¼šçˆ¬è™«çš„åå­—ï¼Œå¿…é¡»å”¯ä¸€ï¼ˆå¦‚æœåœ¨æ§åˆ¶å°ä½¿ç”¨çš„è¯ï¼Œå¿…é¡»é…ç½®ï¼‰
- `start_urls`ï¼šçˆ¬è™«åˆå§‹çˆ¬å–çš„é“¾æ¥åˆ—è¡¨
- `custom_settings`ï¼šè‡ªå®šä¹‰é…ç½®ï¼Œè¦†ç›–`settings.py`ä¸­çš„é»˜è®¤é…ç½®


- `start_requests`ï¼šå¯åŠ¨çˆ¬è™«çš„æ—¶å€™è°ƒç”¨ï¼Œé»˜è®¤æ˜¯è°ƒç”¨`make_requests_from_url`æ–¹æ³•çˆ¬å–`start_urls`çš„é“¾æ¥ï¼Œå¯ä»¥åœ¨è¿™ä¸ªæ–¹æ³•é‡Œé¢å®šåˆ¶ï¼Œå¦‚æœé‡å†™äº†è¯¥æ–¹æ³•ï¼Œstart_urlsé»˜è®¤å°†ä¸ä¼šè¢«ä½¿ç”¨ï¼Œå¯ä»¥åœ¨è¿™ä¸ªæ–¹æ³•é‡Œé¢å®šåˆ¶ä¸€äº›è‡ªå®šä¹‰çš„urlï¼Œå¦‚ç™»å½•ï¼Œä»æ•°æ®åº“è¯»å–urlç­‰ï¼Œæœ¬æ–¹æ³•è¿”å›Requestå¯¹è±¡
- `make_requests_from_url`ï¼šé»˜è®¤ç”±`start_requests`è°ƒç”¨ï¼Œå¯ä»¥é…ç½®Requestå¯¹è±¡ï¼Œè¿”å›Requestå¯¹è±¡
- `parse`ï¼šresponseåˆ°è¾¾spiderçš„æ—¶å€™é»˜è®¤è°ƒç”¨ï¼Œå¦‚æœåœ¨Requestå¯¹è±¡é…ç½®äº†callbackå‡½æ•°ï¼Œåˆ™ä¸ä¼šè°ƒç”¨ï¼Œparseæ–¹æ³•å¯ä»¥è¿­ä»£è¿”å›`Item`æˆ–`Request`å¯¹è±¡ï¼Œå¦‚æœè¿”å›Requestå¯¹è±¡ï¼Œåˆ™ä¼šè¿›è¡Œå¢é‡çˆ¬å–

2ã€Requestä¸Responseå¯¹è±¡

- æ¯ä¸ªè¯·æ±‚éƒ½æ˜¯ä¸€ä¸ªRequestå¯¹è±¡ï¼ŒRequestå¯¹è±¡å®šä¹‰äº†è¯·æ±‚çš„ç›¸å…³ä¿¡æ¯ï¼ˆ`url`, `method`, `headers`, `body`, `cookie`, `priority`ï¼‰å’Œå›è°ƒçš„ç›¸å…³ä¿¡æ¯ï¼ˆ`meta`, `callback`, `dont_filter`, `errback`ï¼‰ï¼Œé€šå¸¸ç”±spiderè¿­ä»£è¿”å›ï¼›å…¶ä¸­`meta`ç›¸å½“äºé™„åŠ å˜é‡ï¼Œå¯ä»¥åœ¨è¯·æ±‚å®Œæˆåé€šè¿‡`response.meta`è®¿é—®
- è¯·æ±‚å®Œæˆåï¼Œä¼šé€šè¿‡`Response`å¯¹è±¡å‘é€ç»™spiderå¤„ç†ï¼Œå¸¸ç”¨å±æ€§æœ‰ï¼ˆ`url`, `status`, `headers`, `body`, `request`, `meta`, ï¼‰

è¯¦ç»†ä»‹ç»å‚è€ƒå®˜ç½‘ï¼š[request-objects](https://doc.scrapy.org/en/latest/topics/request-response.html#request-objects)  [response-object](https://doc.scrapy.org/en/latest/topics/request-response.html#response-objects)

çœ‹çœ‹ä¸‹é¢çš„ä¾‹å­ï¼š

```python
from scrapy import Spider
from scrapy import Request
 
class TestSpider(Spider):
    name = 'test'
    start_urls = ["http://www.qq.com/",]
 
    def login_parse(self, response):
        # å¦‚æœç™»å½•æˆåŠŸ,æ‰‹åŠ¨æ„é€ è¯·æ±‚Requestè¿­ä»£è¿”å›
        print response
        for i in range(0, 10):
            yield Request('http://www.example.com/list/1?page={0}'.format(i))
 
    def start_requests(self):
        # è¦†ç›–é»˜è®¤çš„æ–¹æ³•(å¿½ç•¥start_urls),è¿”å›ç™»å½•è¯·æ±‚é¡µ,åˆ¶å®šå¤„ç†å‡½æ•°ä¸ºlogin_parse
        return Request('http://www.example.com/login', method="POST" body='username=bomo&pwd=123456', callback=self.login_parse)
 
    def parse(self, response):
        #é»˜è®¤è¯·æ±‚å¤„ç†å‡½æ•°
        print response
```

3ã€Selectoré€‰æ‹©å™¨

csså’Œxpathè‡ªå·±é€‰æ‹©å³å¯ï¼Œå½“ç„¶è¿˜æœ‰reï¼Œextractæ–¹æ³•å¯ä»¥æå–`data`å±æ€§å€¼ï¼›

å…¶ä»–å‚è€ƒ[å®˜ç½‘](https://doc.scrapy.org/en/latest/topics/selectors.html)

#### bã€å…³äºPipeline

itemå£°æ˜ç»“æ„åŒ–æ•°æ®ï¼Œspiderè´Ÿè´£çˆ¬å–æ•°æ®ï¼Œç„¶åäº¤ç»™Pipelineå¤„ç†ï¼ˆå¤„ç†ï¼Œè¿‡æ»¤ï¼Œä¿å­˜ï¼‰ï¼Œå¯ä»¥åˆ›å»ºå¤šä¸ªPipelineï¼Œåˆ›å»ºçš„Pipelineè®°å¾—åœ¨settingsä¸­æ¿€æ´»/æ’åºï¼ˆä¼˜å…ˆçº§ï¼‰ã€‚å¸¸è§çš„å¤„ç†å¦‚ä¸‹ï¼š

- cleansing HTML data
- validating scraped data (checking that the items contain certain fields)
- checking for duplicates (and dropping them)
- storing the scraped item in a database

æ¯ä¸ªPipelineéƒ½æ˜¯ä¸€ä¸ªç±»ï¼Œå¿…é¡»å®ç°å¦‚ä¸‹æ–¹æ³•ï¼š

`process_item`(*self*, *item*, *spider*)

å…¶ä»–æ–¹æ³•å¯é€‰ï¼š

- `open_spider`(*self*, *spider*)


- `close_spider`(*self*, *spider*)


- `from_crawler`(*cls*, *crawler*)

  If present, this classmethod is called to create a pipeline instance from a [`Crawler`](https://doc.scrapy.org/en/latest/topics/api.html#scrapy.crawler.Crawler). It must return a new instance of the pipeline. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for pipeline to access them and hook its functionality into Scrapy.

æ›´å¤šè¯·å‚è€ƒ[å®˜ç½‘](https://doc.scrapy.org/en/latest/topics/item-pipeline.html)

å®ä¾‹å‚è€ƒå¦‚ä¸‹ï¼š

```python
# å†™å…¥MongoDB
import pymongo

class MongoPipeline(object):

    collection_name = 'scrapy_items'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(dict(item))
        return item
```



#### cã€CrawlSpiderå’Œrules

çˆ¬è™«çš„é€šå¸¸éœ€è¦åœ¨ä¸€ä¸ªç½‘é¡µé‡Œé¢çˆ¬å–å…¶ä»–çš„é“¾æ¥ï¼Œç„¶åä¸€å±‚ä¸€å±‚å¾€ä¸‹çˆ¬ï¼Œscrapyæä¾›äº†LinkExtractorç±»ç”¨äºå¯¹ç½‘é¡µé“¾æ¥çš„æå–ï¼Œä½¿ç”¨LinkExtractoréœ€è¦ä½¿ç”¨`CrawlSpider`çˆ¬è™«ç±»ä¸­ï¼Œ`CrawlSpider`ä¸`Spider`ç›¸æ¯”ä¸»è¦æ˜¯å¤šäº†`rules`ï¼Œå¯ä»¥æ·»åŠ ä¸€äº›è§„åˆ™ï¼Œå…ˆçœ‹ä¸‹é¢è¿™ä¸ªä¾‹å­ï¼Œçˆ¬å–é“¾å®¶ç½‘çš„é“¾æ¥ã€‚

> é€‚åˆæ•´ç«™çˆ¬å–ï¼Œå…·ä½“å‚è€ƒ[å®˜ç½‘](https://doc.scrapy.org/en/latest/topics/spiders.html)

```python
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
 
class LianjiaSpider(CrawlSpider):
    name = "lianjia"
 
    allowed_domains = ["lianjia.com"]
 
    start_urls = [
        "http://bj.lianjia.com/ershoufang/"
    ]
 
    rules = [
        # åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼,å¤„ç†ä¸‹ä¸€é¡µ
        Rule(LinkExtractor(allow=(r'http://bj.lianjia.com/ershoufang/pg\s+$',)), callback='parse_item'),
 
        # åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼,ç»“æœåŠ åˆ°urlåˆ—è¡¨ä¸­,è®¾ç½®è¯·æ±‚é¢„å¤„ç†å‡½æ•°
        # Rule(FangLinkExtractor(allow=('http://www.lianjia.com/client/', )), follow=True, process_request='add_cookie')
    ]
 
    def parse_item(self, response):
        # è¿™é‡Œä¸ä¹‹å‰çš„parseæ–¹æ³•ä¸€æ ·ï¼Œå¤„ç†
        pass
```

#### dã€Middleware

`SpiderMiddleware`ï¼šé€šå¸¸ç”¨äºé…ç½®çˆ¬è™«ç›¸å…³çš„å±æ€§ï¼Œå¼•ç”¨é“¾æ¥è®¾ç½®ï¼ŒUrlé•¿åº¦é™åˆ¶ï¼ŒæˆåŠŸçŠ¶æ€ç è®¾ç½®ï¼Œçˆ¬å–æ·±åº¦è®¾ç½®ï¼Œçˆ¬å»ä¼˜å…ˆçº§è®¾ç½®ç­‰

`DownloadMiddlware`ï¼šé€šå¸¸ç”¨äºå¤„ç†ä¸‹è½½ä¹‹å‰çš„é¢„å¤„ç†ï¼Œå¦‚è¯·æ±‚Headerï¼ˆCookie,User-Agentï¼‰ï¼Œç™»å½•éªŒè¯å¤„ç†ï¼Œé‡å®šå‘å¤„ç†ï¼Œä»£ç†æœåŠ¡å™¨å¤„ç†ï¼Œè¶…æ—¶å¤„ç†ï¼Œé‡è¯•å¤„ç†ç­‰

**1ã€SpiderMiddleware**

çˆ¬è™«ä¸­é—´ä»¶æœ‰ä¸‹é¢å‡ ä¸ªæ–¹æ³•

- `process_spider_input`ï¼šå½“responseé€šè¿‡spiderçš„æ—¶å€™è¢«è°ƒç”¨ï¼Œè¿”å›Noneï¼ˆç»§ç»­ç»™å…¶ä»–ä¸­é—´ä»¶å¤„ç†ï¼‰æˆ–æŠ›å‡ºå¼‚å¸¸ï¼ˆä¸ä¼šç»™å…¶ä»–ä¸­é—´ä»¶å¤„ç†ï¼Œå½“æˆå¼‚å¸¸å¤„ç†ï¼‰
- `process_spider_output`ï¼šå½“spideræœ‰itemæˆ–Requestè¾“å‡ºçš„æ—¶å€™è°ƒç”¨
- `process_spider_exception`ï¼šå¤„ç†å‡ºç°å¼‚å¸¸æ—¶è°ƒç”¨
- `process_start_requests`ï¼šspiderå½“å¼€å§‹è¯·æ±‚Requestçš„æ—¶å€™è°ƒç”¨

ä¸‹é¢æ˜¯scrapyè‡ªå¸¦çš„ä¸€äº›ä¸­é—´ä»¶ï¼ˆåœ¨`scrapy.spidermiddlewares`å‘½åç©ºé—´ä¸‹ï¼‰

- UrlLengthMiddleware
- RefererMiddleware
- OffsiteMiddleware
- HttpErrorMiddleware
- DepthMiddleware

> å…¶ä»–å‚è€ƒ[å®˜ç½‘](http://doc.scrapy.org/en/latest/topics/spider-middleware.html)

**2ã€DownloaderMiddleware**

ä¸‹è½½ä¸­é—´ä»¶æœ‰ä¸‹é¢å‡ ä¸ªæ–¹æ³•

- `process_request`ï¼šè¯·æ±‚é€šè¿‡ä¸‹è½½å™¨çš„æ—¶å€™è°ƒç”¨
- `process_response`ï¼šè¯·æ±‚å®Œæˆåè°ƒç”¨
- `process_exception`ï¼šè¯·æ±‚å‘ç”Ÿå¼‚å¸¸æ—¶è°ƒç”¨
- `from_crawler`ï¼šä»crawleræ„é€ çš„æ—¶å€™è°ƒç”¨
- `from_settings`ï¼šä»settingsæ„é€ çš„æ—¶å€™è°ƒç”¨

> æ›´å¤šè¯¦ç»†çš„å‚æ•°è§£é‡Šè§[è¿™é‡Œ](https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/downloader-middleware.html#id2)

åœ¨çˆ¬å–ç½‘é¡µçš„æ—¶å€™ï¼Œä½¿ç”¨ä¸åŒçš„`User-Agent`å¯ä»¥æé«˜è¯·æ±‚çš„éšæœºæ€§ï¼Œå®šä¹‰ä¸€ä¸ªéšæœºè®¾ç½®User-Agentçš„ä¸­é—´ä»¶`RandomUserAgentMiddleware

```python
import random

class RandomUserAgentMiddleware(object):
    """Randomly rotate user agents based on a list of predefined ones"""

    def __init__(self, agents):
        self.agents = agents

    # ä»crawleræ„é€ ï¼ŒUSER_AGENTSå®šä¹‰åœ¨crawlerçš„é…ç½®çš„è®¾ç½®ä¸­
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings.getlist('USER_AGENTS'))

    # ä»settingsæ„é€ ï¼ŒUSER_AGENTSå®šä¹‰åœ¨settings.pyä¸­
    @classmethod
    def from_settings(cls, settings):
        return cls(settings.getlist('USER_AGENTS'))

    def process_request(self, request, spider):
        # è®¾ç½®éšæœºçš„User-Agent
        request.headers.setdefault('User-Agent', random.choice(self.agents))
```

åœ¨`settings.py`è®¾ç½®USER_AGENTSå‚æ•°

```python
USER_AGENTS = [
    "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
    ...
]
```

é…ç½®çˆ¬è™«ä¸­é—´ä»¶çš„æ–¹å¼ä¸pipelineç±»ä¼¼ï¼Œç¬¬äºŒä¸ªå‚æ•°è¡¨ç¤ºä¼˜å…ˆçº§

```python
# é…ç½®çˆ¬è™«ä¸­é—´ä»¶
SPIDER_MIDDLEWARES = {
    'myproject.middlewares.CustomSpiderMiddleware': 543,
    # å¦‚æœæƒ³ç¦ç”¨é»˜è®¤çš„ä¸­é—´ä»¶çš„è¯ï¼Œå¯ä»¥è®¾ç½®å…¶ä¼˜å…ˆçº§ä¸ºNone
    'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': None,
}

# é…ç½®ä¸‹è½½ä¸­é—´ä»¶
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.RandomUserAgentMiddleware': 543,
    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,
}
```

#### eã€ç¼“å­˜

scrapyé»˜è®¤å·²ç»è‡ªå¸¦äº†ç¼“å­˜çš„åŠŸèƒ½ï¼Œé€šå¸¸æˆ‘ä»¬åªéœ€è¦é…ç½®å³å¯ï¼Œæ‰“å¼€`settings.py`

```python
# æ‰“å¼€ç¼“å­˜
HTTPCACHE_ENABLED = True
 
# è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå•ä½ï¼šç§’ï¼‰
#HTTPCACHE_EXPIRATION_SECS = 0
 
# ç¼“å­˜è·¯å¾„(é»˜è®¤ä¸ºï¼š.scrapy/httpcache)
HTTPCACHE_DIR = 'httpcache'
 
# å¿½ç•¥çš„çŠ¶æ€ç 
HTTPCACHE_IGNORE_HTTP_CODES = []
 
# ç¼“å­˜æ¨¡å¼(æ–‡ä»¶ç¼“å­˜)
HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
```

> æ›´å¤šå‚æ•°å‚è§[è¿™é‡Œ](http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings)

#### fã€å¤šçº¿ç¨‹

scrapyç½‘ç»œè¯·æ±‚æ˜¯åŸºäºTwistedï¼Œè€ŒTwistedé»˜è®¤æ”¯æŒå¤šçº¿ç¨‹ï¼Œè€Œä¸”scrapyé»˜è®¤ä¹Ÿæ˜¯é€šè¿‡å¤šçº¿ç¨‹è¯·æ±‚çš„ï¼Œå¹¶ä¸”æ”¯æŒå¤šæ ¸CPUçš„å¹¶å‘ï¼Œé€šå¸¸åªéœ€è¦é…ç½®ä¸€äº›å‚æ•°å³å¯

```python
# é»˜è®¤Itemå¹¶å‘æ•°ï¼š100
CONCURRENT_ITEMS = 100
 
# é»˜è®¤Requestå¹¶å‘æ•°ï¼š16
CONCURRENT_REQUESTS = 16
 
# é»˜è®¤æ¯ä¸ªåŸŸåçš„å¹¶å‘æ•°ï¼š8
CONCURRENT_REQUESTS_PER_DOMAIN = 8
 
# æ¯ä¸ªIPçš„æœ€å¤§å¹¶å‘æ•°ï¼š0è¡¨ç¤ºå¿½ç•¥
CONCURRENT_REQUESTS_PER_IP = 0
```

> æ›´å¤šå‚æ•°å‚è§[è¿™é‡Œ](https://doc.scrapy.org/en/latest/topics/settings.html#concurrent-items)

### 5.4ã€è¿›é˜¶

#### aã€ä¸‹è½½æ–‡ä»¶/å›¾ç‰‡

é¡¹ç›®å‚è€ƒï¼šDouyuImage_scrapy



### 5.4ã€å¸¸è§é—®é¢˜

#### aã€é¡¹ç›®åç§°é—®é¢˜

åœ¨ä½¿ç”¨çš„æ—¶å€™é‡åˆ°è¿‡ä¸€ä¸ªé—®é¢˜ï¼Œåœ¨åˆå§‹åŒ–`scrapy startproject tutorial`çš„æ—¶å€™ï¼Œå¦‚æœä½¿ç”¨äº†ä¸€äº›ç‰¹æ®Šçš„åå­—ï¼Œå¦‚ï¼š`test`, `fang`ç­‰å•è¯çš„è¯ï¼Œé€šè¿‡`get_project_settings`æ–¹æ³•è·å–é…ç½®çš„æ—¶å€™ä¼šå‡ºé”™ï¼Œæ”¹æˆ`tutorial`æˆ–ä¸€äº›å¤æ‚çš„åå­—çš„æ—¶å€™ä¸ä¼š

```python
ImportError: No module named tutorial.settings
```

è¿™æ˜¯ä¸€ä¸ªbugï¼Œåœ¨githubä¸Šæœ‰æåˆ°ï¼š<https://github.com/scrapy/scrapy/issues/428>ï¼Œä½†è²Œä¼¼æ²¡æœ‰å®Œå…¨ä¿®å¤ï¼Œä¿®æ”¹ä¸€ä¸‹åå­—å°±å¥½äº†ï¼ˆå½“ç„¶`scrapy.cfg`å’Œ`settings.py`é‡Œé¢ä¹Ÿéœ€è¦ä¿®æ”¹ï¼‰

#### bã€ä¸ºæ¯ä¸ªpipelineé…ç½®spider

ä¸Šé¢æˆ‘ä»¬æ˜¯åœ¨settings.pyé‡Œé¢é…ç½®pipelineï¼Œè¿™é‡Œçš„é…ç½®çš„pipelineä¼šä½œç”¨äºæ‰€æœ‰çš„spiderï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸€ä¸ªspideré…ç½®ä¸åŒçš„pipelineï¼Œè®¾ç½®`Spider`çš„`custom_settings`å¯¹è±¡

```python
class LianjiaSpider(CrawlSpider):
    ...
    # è‡ªå®šä¹‰é…ç½®
    custom_settings = {
        'ITEM_PIPELINES': {
            'tutorial.pipelines.TestPipeline.TestPipeline': 1,
        }
    }
```

#### cã€è·å–æå–é“¾æ¥çš„èŠ‚ç‚¹ä¿¡æ¯

é€šè¿‡LinkExtractoræå–çš„`scrapy.Link`é»˜è®¤ä¸å¸¦èŠ‚ç‚¹ä¿¡æ¯ï¼Œæœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦èŠ‚ç‚¹çš„å…¶ä»–attributeå±æ€§ï¼Œ`scrapy.Link`æœ‰ä¸ª`text`å±æ€§ä¿å­˜ä»èŠ‚ç‚¹æå–çš„`text`å€¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹`lxmlhtml._collect_string_content`å˜é‡ä¸º`etree.tostring`ï¼Œè¿™æ ·å¯ä»¥åœ¨æå–èŠ‚ç‚¹å€¼å°±å˜å‘³æ¸²æŸ“èŠ‚ç‚¹`scrapy.Link.text`ï¼Œç„¶åæ ¹æ®`scrapy.Link.text`å±æ€§æ‹¿åˆ°èŠ‚ç‚¹çš„htmlï¼Œæœ€åæå–å‡ºæˆ‘ä»¬éœ€è¦çš„å€¼

```python
from lxml import etree
import scrapy.linkextractors.lxmlhtml
scrapy.linkextractors.lxmlhtml._collect_string_content = etree.tostring
```

#### dã€ä»æ•°æ®åº“ä¸­è¯»å–urls

æœ‰æ—¶å€™æˆ‘ä»¬å·²ç»æŠŠurlsä¸‹è½½åˆ°æ•°æ®åº“äº†ï¼Œè€Œä¸æ˜¯åœ¨start_urlsé‡Œé…ç½®ï¼Œè¿™æ—¶å€™å¯ä»¥é‡è½½spiderçš„`start_requests`æ–¹æ³•

```python
def start_requests(self):
    for u in self.db.session.query(User.link):
        yield Request(u.link)
```

æˆ‘ä»¬è¿˜å¯ä»¥åœ¨Requestæ·»åŠ å…ƒæ•°æ®ï¼Œç„¶ååœ¨responseä¸­è®¿é—®

```python
def start_requests(self):
    for u in self.db.session.query(User):
        yield Request(u.link, meta={'name': u.name})
 
def parse(self, response):
    print response.url, response.meta['name']
```

#### eã€å¦‚ä½•è¿›è¡Œå¾ªç¯çˆ¬å–

æœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦çˆ¬å–çš„ä¸€äº›ç»å¸¸æ›´æ–°çš„é¡µé¢ï¼Œä¾‹å¦‚ï¼šé—´éš”æ—¶é—´ä¸º2sï¼Œçˆ¬å»ä¸€ä¸ªåˆ—è¡¨å‰10é¡µçš„æ•°æ®ï¼Œä»ç¬¬ä¸€é¡µå¼€å§‹çˆ¬ï¼Œçˆ¬å®Œæˆåé‡æ–°å›åˆ°ç¬¬ä¸€é¡µ

ç›®å‰çš„æ€è·¯æ˜¯ï¼Œé€šè¿‡parseæ–¹æ³•è¿­ä»£è¿”å›Requestè¿›è¡Œå¢é‡çˆ¬å–ï¼Œç”±äºscrapyé»˜è®¤ç”±ç¼“å­˜æœºåˆ¶ï¼Œéœ€è¦ä¿®æ”¹

#### fã€å…³äºå»é‡

scrapyé»˜è®¤æœ‰è‡ªå·±çš„å»é‡æœºåˆ¶ï¼Œé»˜è®¤ä½¿ç”¨`scrapy.dupefilters.RFPDupeFilter`ç±»è¿›è¡Œå»é‡ï¼Œä¸»è¦é€»è¾‘å¦‚ä¸‹

```python
if include_headers:
    include_headers = tuple(to_bytes(h.lower())
                             for h in sorted(include_headers))
cache = _fingerprint_cache.setdefault(request, {})
if include_headers not in cache:
    fp = hashlib.sha1()
    fp.update(to_bytes(request.method))
    fp.update(to_bytes(canonicalize_url(request.url)))
    fp.update(request.body or b'')
    if include_headers:
        for hdr in include_headers:
            if hdr in request.headers:
                fp.update(hdr)
                for v in request.headers.getlist(hdr):
                    fp.update(v)
    cache[include_headers] = fp.hexdigest()
return cache[include_headers]
```

é»˜è®¤çš„å»é‡æŒ‡çº¹æ˜¯sha1(method + url + body + header)ï¼Œè¿™ç§æ–¹å¼å¹¶ä¸èƒ½è¿‡æ»¤å¾ˆå¤šï¼Œä¾‹å¦‚æœ‰ä¸€äº›è¯·æ±‚ä¼šåŠ ä¸Šæ—¶é—´æˆ³çš„ï¼ŒåŸºæœ¬æ¯æ¬¡éƒ½ä¼šä¸åŒï¼Œè¿™æ—¶å€™æˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰è¿‡æ»¤è§„åˆ™

```python
from scrapy.dupefilter import RFPDupeFilter
 
class CustomURLFilter(RFPDupeFilter):
    """ åªæ ¹æ®urlå»é‡"""
 
    def __init__(self, path=None):
        self.urls_seen = set()
        RFPDupeFilter.__init__(self, path)
 
    def request_seen(self, request):
        if request.url in self.urls_seen:
            return True
        else:
            self.urls_seen.add(request.url)
```

é…ç½®setting

```python
DUPEFILTER_CLASS = 'tutorial.custom_filters.CustomURLFilter'
```

#### gã€Pipelineå¤„ç†ä¸åŒçš„Item

scrapyæ‰€æœ‰çš„è¿­ä»£å‡ºæ¥çš„çš„Iteméƒ½ä¼šç»è¿‡æ‰€æœ‰çš„Pipelineï¼Œå¦‚æœéœ€è¦å¤„ç†ä¸åŒçš„Itemï¼Œåªèƒ½é€šè¿‡`isinstance()`æ–¹æ³•è¿›è¡Œç±»å‹åˆ¤æ–­ï¼Œç„¶ååˆ†åˆ«è¿›è¡Œå¤„ç†ï¼Œæš‚æ—¶æ²¡æœ‰æ›´å¥½çš„æ–¹æ¡ˆ

#### hã€urlæŒ‰é¡ºåºæ‰§è¡Œ

æˆ‘ä»¬å¯ä»¥é€šè¿‡Requestçš„priorityæ§åˆ¶urlçš„è¯·æ±‚çš„æ‰§è¡Œé¡ºåºï¼Œä½†ç”±äºç½‘ç»œè¯·æ±‚çš„ä¸ç¡®å®šæ€§ï¼Œä¸èƒ½ä¿è¯è¿”å›ä¹Ÿæ˜¯æŒ‰ç…§é¡ºåºè¿›è¡Œçš„ï¼Œå¦‚æœéœ€è¦è¿›è¡Œé€ä¸ªurlè¯·æ±‚çš„è¯ï¼Œå§urlåˆ—è¡¨æ”¾åœ¨metaå¯¹è±¡é‡Œé¢ï¼Œåœ¨responseçš„æ—¶å€™è¿­ä»£è¿”å›ä¸‹ä¸€ä¸ªRequestå¯¹è±¡åˆ°è°ƒåº¦å™¨ï¼Œè¾¾åˆ°é¡ºåºæ‰§è¡Œçš„ç›®çš„ï¼Œæš‚æ—¶æ²¡æœ‰æ›´å¥½çš„æ–¹æ¡ˆ

## 