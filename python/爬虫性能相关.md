# 爬虫性能相关

参考：http://www.cnblogs.com/wupeiqi/articles/6229292.html

I/O密集型：多线程（爬虫）

计算密集型：多进程

## 1 多线程实现：

可以实现并发。

但是，请求发出去和返回之前，中间时期进程空闲。

编写方式：

	- 直接返回处理
- 通过回调函数处理

### 1.1 错误的并发：

依然是一个一个执行

```python
from concurrent.futures import ThreadPoolExecutor
import requests
pool = ThreadPoolExecutor(5)

url_list = [
    'http://www.baidu.com'
    'http://www.zhihu.com'
    'http://www.cnblogs.com'
    'http://www.sina.com'
    'http://www.bing.com'
    'http://www.163.com'
]

for url in url_list:
    response = requests.get(url) # 还是一个一个执行
```

### 1.2 正确的并发1：

直接返回数据。

```python
from concurrent.futures import ThreadPoolExecutor
import requests

def task(url):
    response = requests.get(url)
    print(url, response)
    
pool = ThreadPoolExecutor(5)

url_list = [
    'http://www.baidu.com'
    'http://www.zhihu.com'
    'http://www.cnblogs.com'
    'http://www.sina.com'
    'http://www.bing.com'
    'http://www.163.com'
]

for url in url_list:
    pool.submit(task, url)
    
pool.shutdown(wait=True)
```

### 1.3 改进版的并发：

通过回调函数处理，降低耦合。

```python
from concurrent.futures import ThreadPoolExecutor
import requests

def task(url):
    response = requests.get(url)
    # print(url, response)
    return response

def done1(future, *args, **kwargs):
    response = future.result()
    print(response.status_code, response.content)
    
    
pool = ThreadPoolExecutor(5)

url_list = [
    'http://www.baidu.com'
    'http://www.zhihu.com'
    'http://www.cnblogs.com'
    'http://www.sina.com'
    'http://www.bing.com'
    'http://www.163.com'
]

for url in url_list:
    v = pool.submit(task, url) # 添加请求任务，接受返回值
    v.add_done_callback(done1) # 添加解析任务
    # task完成之后，自动执行done1，并把task的返回值传递给done1参数。
    # v.add_done_callback(done2) # 添加入库任务
    #  ...其他任务
    
pool.shutdown(wait=True)
```

### 1.4 问题

但是，如下可以看出：请求发出后和返回之前，中间时期线程空闲。

```python
from concurrent.futures import ThreadPoolExecutor
import requests
import time

def task(url):
    # response = requests.get(url)
    # print(url, response)
    time.sleep(1)
    
pool = ThreadPoolExecutor(5)

url_list = [
    'http://www.baidu.com'
    'http://www.zhihu.com'
    'http://www.cnblogs.com'
    'http://www.sina.com'
    'http://www.bing.com'
    'http://www.163.com'
]

for url in url_list:
    pool.submit(task, url)
    
pool.shutdown(wait=True)
```

## 2 多进程实现：

与多线程类似，可以实现并发。

但是，请求发出去和返回之前，中间时期进程空闲。

编写方式：

- 直接返回处理
- 通过回调函数处理

### 2.1 正确的并发1：

直接返回数据。

```python
from concurrent.futures import ThreadPoolExecutor
import requests

def task(url):
    response = requests.get(url)
    print(url, response)
    
pool = ThreadPoolExecutor(5)

url_list = [
    'http://www.baidu.com'
    'http://www.zhihu.com'
    'http://www.cnblogs.com'
    'http://www.sina.com'
    'http://www.bing.com'
    'http://www.163.com'
]

for url in url_list:
    pool.submit(task, url)
    
pool.shutdown(wait=True)
```

### 2.2 改进版的并发：

通过回调函数处理，降低耦合。

```python
from concurrent.futures import ThreadPoolExecutor
import requests

def task(url):
    response = requests.get(url)
    # print(url, response)
    return response

def done1(future, *args, **kwargs):
    response = future.result()
    print(response.status_code, response.content)
    
    
pool = ThreadPoolExecutor(5)

url_list = [
    'http://www.baidu.com'
    'http://www.zhihu.com'
    'http://www.cnblogs.com'
    'http://www.sina.com'
    'http://www.bing.com'
    'http://www.163.com'
]

for url in url_list:
    v = pool.submit(task, url) # 添加请求任务，接受返回值
    v.add_done_callback(done1) # 添加解析任务
    # task完成之后，自动执行done1，并把task的返回值传递给done1参数。
    # v.add_done_callback(done2) # 添加入库任务
    #  ...其他任务
    
pool.shutdown(wait=True)
```

### 2.3 问题

但是，如下可以看出：请求发出后和返回之前，中间时期进程空闲。

```python
from concurrent.futures import ProcessPoolExecutor
import requests
import time

def task(url):
    # response = requests.get(url)
    # print(url, response)
    time.sleep(1)
    
pool = ProcessPoolExecutor(5)

url_list = [
    'http://www.baidu.com'
    'http://www.zhihu.com'
    'http://www.cnblogs.com'
    'http://www.sina.com'
    'http://www.bing.com'
    'http://www.163.com'
]

for url in url_list:
    pool.submit(task, url)
    
pool.shutdown(wait=True)
```

## 3 协程（微线程）+异步I/O

通过上述代码均可以完成对请求性能的提高，对于多线程和多进行的缺点是在IO阻塞时会造成了线程和进程的浪费，所以异步IO回事首选：

协程（微线程）：一个进程中的一个线程在多个执行函数之间来回切换，并能记住之前的状态继续执行。**只是在函数之间来回切换的功能**。

协程 + 异步I/O:1个线程发送N个Http请求，

### 3.1 asyncio

asyncio：为第三方模块，内部实现了异步I/O

#### 3.1.1 asyncio示例1

```python
import asyncio

@asyncio.coroutine
def func1():
    print('before...func1......')
    yield from asyncio.sleep(5)  
    # 但是asyncio不支持http，只支持tcp。
    print('end...func1......')


tasks = [func1(), func1()]

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.gather(*tasks))
loop.close()
```

#### 3.1.1 asyncio示例2

封装http数据包：我们可以想办法把http封装成tcp请求：

```python
import asyncio


@asyncio.coroutine
def fetch_async(host, url='/'):
    print(host, url)
    reader, writer = yield from asyncio.open_connection(host, 80)

    request_header_content = """GET %s HTTP/1.0\r\nHost: %s\r\n\r\n""" % (url, host,)
    request_header_content = bytes(request_header_content, encoding='utf-8')

    writer.write(request_header_content)
    yield from writer.drain()
    text = yield from reader.read()
    print(host, url, text)
    writer.close()

tasks = [
    fetch_async('www.cnblogs.com', '/wupeiqi/'),
    fetch_async('dig.chouti.com', '/pic/show?nid=4073644713430508&lid=10273091')
]

loop = asyncio.get_event_loop()
results = loop.run_until_complete(asyncio.gather(*tasks))
loop.close()
```

但是这样写的代码好多。。。于是，有人封装成了aiohttp模块。

#### 3.1.2 asyncio + aiohttp

aiohttp:封装http数据包

需要先安装aiohttp模块：

```python
pip install aiohttp
```



```python
import aiohttp  # 把http包装成tcp，还有其他功能
import asyncio  # 异步I/O

@asyncio.coroutine
def fetch_async(url):
    print(url)
    response = yield from aiohttp.request('GET', url)
    # data = yield from response.read()
    # print(url, data)
    print(url, response)
    response.close()


tasks = [fetch_async('http://www.google.com/'), fetch_async('http://www.chouti.com/')]

event_loop = asyncio.get_event_loop()
results = event_loop.run_until_complete(asyncio.gather(*tasks))
event_loop.close()
```

#### 3.1.3 asyncio + requests

requests:封装http数据包

需要先安装requests模块：

```python
pip install requests
```



```python
import asyncio
import requests


@asyncio.coroutine
def fetch_async(func, *args):
    loop = asyncio.get_event_loop()
    future = loop.run_in_executor(None, func, *args)
    response = yield from future
    print(response.url, response.content)


tasks = [
    fetch_async(requests.get, 'http://www.cnblogs.com/wupeiqi/'),
    fetch_async(requests.get, 'http://dig.chouti.com/pic/show?nid=4073644713430508&lid=10273091')
]

loop = asyncio.get_event_loop()
results = loop.run_until_complete(asyncio.gather(*tasks))
loop.close()
```

### 3.2 gevent+ requests

是greenlet的进一步封装，内部实现了greenlet（协程）和异步I/O。

greenlet也是建立在socket层级上，所以也需要requests进行http数据封装。

```python
pip install greenlet
pip install gevent
```

#### 3.2.1 gevent+ requests实例1

```python
import gevent

import requests
from gevent import monkey

monkey.patch_all()


def fetch_async(method, url, req_kwargs):
    print(method, url, req_kwargs)
    response = requests.request(method=method, url=url, **req_kwargs)
    print(response.url, response.content)

# ##### 发送请求 #####
gevent.joinall([
    gevent.spawn(fetch_async, method='get', url='https://www.python.org/', req_kwargs={}),
    gevent.spawn(fetch_async, method='get', url='https://www.yahoo.com/', req_kwargs={}),
    gevent.spawn(fetch_async, method='get', url='https://github.com/', req_kwargs={}),
])
```

#### 3.2.2 gevent+ requests实例2

使用协程池控制最大发送多少个请求。

```python
import gevent

import requests
from gevent import monkey

monkey.patch_all()


def fetch_async(method, url, req_kwargs):
    print(method, url, req_kwargs)
    response = requests.request(method=method, url=url, **req_kwargs)
    print(response.url, response.content)


# ##### 发送请求（协程池控制最大协程数量） #####
from gevent.pool import Pool
pool = Pool(None)  # 指定最多发送几个请求
gevent.joinall([
    pool.spawn(fetch_async, method='get', url='https://www.python.org/', req_kwargs={}),
    pool.spawn(fetch_async, method='get', url='https://www.yahoo.com/', req_kwargs={}),
    pool.spawn(fetch_async, method='get', url='https://www.github.com/', req_kwargs={}),
])
```

#### 3.2.3 gevent+ requests=>grequests

有人简单的把gevent+ requests二者封装在一起形成了grequests。

```sh
pip install grequests
```

```python
import grequests


request_list = [
    grequests.get('http://httpbin.org/delay/1', timeout=0.001),
    grequests.get('http://fakedomain/'),
    grequests.get('http://httpbin.org/status/500')
]


# ##### 执行并获取响应列表 #####
response_list = grequests.map(request_list)
print(response_list)


# ##### 执行并获取响应列表（处理异常） #####
# def exception_handler(request, exception):
# print(request,exception)
#     print("Request failed")

# response_list = grequests.map(request_list, exception_handler=exception_handler)
# print(response_list)
```

### 3.3 Twisted

Scrapy内部的并发下载使用的就是Twisted。

```python
pip install twisted
# python3 支持不是太全面。
```

```python
from twisted.web.client import getPage, defer
from twisted.internet import reactor


def all_done(arg):
    reactor.stop()


def callback(contents):
    print(contents)


deferred_list = []

url_list = ['http://www.bing.com', 'http://www.baidu.com', ]
for url in url_list:
    deferred = getPage(bytes(url, encoding='utf8'))
    deferred.addCallback(callback)
    deferred_list.append(deferred)

dlist = defer.DeferredList(deferred_list)
dlist.addBoth(all_done)

reactor.run() # 死循环
```

### 3.4 Tornado

Tornado既可以作web框架，也可以用于爬虫。

```python
pip install tornado
```



```python
from tornado.httpclient import AsyncHTTPClient
from tornado.httpclient import HTTPRequest
from tornado import ioloop


def handle_response(response):
    """
    处理返回值内容（需要维护计数器，来停止IO循环），调用 ioloop.IOLoop.current().stop()
    :param response: 
    :return: 
    """
    if response.error:
        print("Error:", response.error)
    else:
        print(response.body)


def func():
    url_list = [
        'http://www.baidu.com',
        'http://www.bing.com',
    ]
    for url in url_list:
        print(url)
        http_client = AsyncHTTPClient()
        http_client.fetch(HTTPRequest(url), handle_response)


ioloop.IOLoop.current().add_callback(func)
ioloop.IOLoop.current().start() # 死循环
```

### 3.5 总结及选择

1、 首先我们有很多现成的框架可以选择，Scrapy等等。

2、 假如真的需要我们自定义的话，选择顺序如下：

gevent > Twisted > tornado > asyncio